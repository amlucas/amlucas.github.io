<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link href="css/white.css" type="text/css" rel="stylesheet" />
    <link href="css/codehilite.css" rel="stylesheet" />
    <title>Lucas Amoudruz</title>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  </head>
<body>


<header> <nav>   <ul>     <li> <a href="../index.html"> About </a> </li>     <li> <a href="../publications.html"> Publications </a> </li>     <li> <a href="../software.html"> Software </a> </li>     <li> <a href="../gallery.html"> Gallery </a> </li>     <li> <a href="../blog.html"> Blog </a> </li>   </ul> </nav> </header><h1>Graph Neural Networks to learn an energy potential</h1>
<p>May 23, 2024</p>
<p>Graph neural networks (GNNs) are an increasingly popular approach to learn from complex data structures that can be represented as graphs.
In computational science we can often represent a physical system by a graph:</p>
<ul>
<li>A computational grid is a set of nodes connected with each other.</li>
<li>Molecular dynamics is essentially a list of particles (nodes) interacting with pairwise forces (edges).</li>
<li>Unstructure grids to represent PDEs.</li>
<li>Triangle mesh surfaces to represent membranes.</li>
</ul>
<p>To test this method I generated <script type="math/tex"> N </script> particles in a square 2D domain, with random positions <script type="math/tex">\mathbf{r}_i \sim \mathcal{U}\left([0, L] \times [0, L]\right)</script>, <script type="math/tex">i = 1, 2, \dots, N</script>.
Each particle has an associated feature <script type="math/tex">m_i \sim \mathcal{U}(0.1, 1)</script>.
These are the state of the system.</p>
<p>Now, we define an "energy" associated with each particle, that can be computed from the state of the system:
<script type="math/tex; mode=display">
E_i = \sum\limits_{j \neq i, r_{ij} < r_c} m_i m_j w(r_{ij}),
</script>
where <script type="math/tex">r_c = 1</script> is the cutoff radius, <script type="math/tex">r_{ij}</script> the distance separating particles <script type="math/tex">i</script> and <script type="math/tex">j</script> and <script type="math/tex">w(r)=\max\left(0, 1 - \frac{r}{r_c}\right)</script>.</p>
<p>We show below the training data, 500 particles placed randomly in a square box. The color shows the energy.
<img alt="" src="../images/blog/gnn-local-interactions/dataset.png" /></p>
<p>Let's suppose that we have data about the energy of particles in a given configuration, can we predict the energy of particles in a new situation, without knowing the form of the energy?</p>
<p>The GNN architecture is set as follows:
<script type="math/tex; mode=display">
y_i = \sum\limits_{j: (i,j) \in E} \phi(m_i, m_j, \mathbf{e}_{ij}),
</script>
where <script type="math/tex">y_i</script> is the output at node <script type="math/tex">i</script> and the edge features are the direction between particles and their distance, <script type="math/tex">\mathbf{e}_{ij} = \left(\mathbf{r}_{ij} / r_{ij}, r_{ij}\right)</script>.
The function <script type="math/tex">\phi</script> is a multi layer perceptron.
Note that in this case the direction between particles is not useful, but we assume that we don't know this information.</p>
<p>This is easily implemented using <a href="https://pytorch-geometric.readthedocs.io/en/latest/index.html">pytorch-geometric</a>:</p>
<pre class="codehilite"><code class="language-python">class EdgeConv(MessagePassing):
    def __init__(self, x_channels, e_channels, out_channels):
        super().__init__(aggr='add')
        self.mlp = nn.Sequential(nn.Linear(2 * x_channels + e_channels, 16),
                              nn.ReLU(),
                              nn.Linear(16, out_channels))

    def forward(self, x, edge_index, edge_attr):
        return self.propagate(edge_index, x=x, edge_attr=edge_attr)

    def message(self, x_i, x_j, edge_attr):
        z = torch.cat([x_i, x_j, edge_attr], dim=1)
        return self.mlp(z)


class GCN(torch.nn.Module):
    def __init__(self, x_channels, e_channels, out_channels):
        super().__init__()
        self.conv = EdgeConv(x_channels, e_channels, out_channels)

    def forward(self, data):
        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr
        return self.conv(x, edge_index, edge_attr)
</code></pre>

<p>We train for 20000 epochs using the Adam optimizer with a learning rate of 0.001.
Here are the results, the model tested on a new random configuration:</p>
<p><img alt="" src="../images/blog/gnn-local-interactions/comp_xyE.png" />
<img alt="" src="../images/blog/gnn-local-interactions/comp_E.png" /></p>
<p>The prediction of the energy is very close to the gound truth on the test data.</p>

</body>

</html>

